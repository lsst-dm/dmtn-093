..
  Technote content.

  See https://developer.lsst.io/restructuredtext/style.html
  for a guide to reStructuredText writing.

  Do not put the title, authors or other metadata in this document;
  those are automatically added.

  Use the following syntax for sections:

  Sections
  ========

  and

  Subsections
  -----------

  and

  Subsubsections
  ^^^^^^^^^^^^^^

  To add images, add the image file (png, svg or jpeg preferred) to the
  _static/ directory. The reST syntax for adding the image is

  .. figure:: /_static/filename.ext
     :name: fig-label

     Caption text.

   Run: ``make html`` and ``open _build/html/index.html`` to preview your work.
   See the README at https://github.com/lsst-sqre/lsst-technote-bootstrap or
   this repo's README for more info.

   Feel free to delete this instructional comment.

:tocdepth: 1

.. Please do not modify tocdepth; will be fixed when a new Sphinx theme is shipped.

.. sectnum::

.. TODO: Delete the note below before merging new content to the master branch.

.. note::

   **This technote is not yet published.**

Abstract
=========

We describe the design and implementation of the LSST Alert Distribution System, which provides rapid dissemination of alerts as well as simple filtering.



Alert Serialization
===================

Alerts are packaged using Apache Avro `(https://avro.apache.org
<https://avro.apache.org>`_).
Avro is a framework for data serialization in a compact binary format.
It has been used at scale in both industry and science, and it is the
recommended format for data streamed with Apache Kafka.
Avro is more structured in format than JSON or XML, the currently used
format of VOEvent 2.0.
Libraries for reading and writing Avro are available in many languages,
including Python.

To create an Avro alert packet, data is serialized using a JSON schema,
which defines the data types for each field.
Strict adherence to the schema ensures that data will be correctly
interpreted upon receipt.
Alerts in Avro format can be shipped with or without a schema embedded.
Excluding the schema allow alerts to be more lightweight.
However, passing alerts without a schema embedded means that an
appropriate schema fitting the data is needed upon receipt to interpret the data.
Any changes to the schema must be coordinated with downstream readers.
The schema used for deserializing packets can also be different from the
schema used for writing, which enables receivers to easily remove
or add new fields.
Files can be included in Avro packets as data type "bytes," making it
possible to embed postage stamp cutouts of detected difference image
sources in a much more compact way than the current VOEvent standard.

Each alert is packaged as its own Avro packet, as opposed to wrapping
groups of alerts per visit together.
As alerts are anticipated to arrive independently from the end of
the science pipelines parallelized by CCD, the Kafka platform
acts as a cache before distribution, and individually packaged alerts
makes this process simple.
Additionally, packaging alerts separately allows filters to take
as input an individual alert and pass each alert independently
without having to repackage groups, which makes chaining filters
straightforward.

Avro schemas can be composed of nested sub-schemas under a top
level namespace.
Nesting simplifies what would otherwise be monolithic schemas
as new fields are added.
For example, the base alert schema (``lsst.alert``) is of type
"record" and includes previous detections of DIA sources as an array
of type ``lsst.alert.diaSource``.

The following shows the top level alert schema:

.. code-block:: JSON

  {
	"namespace": "lsst",
	"type": "record",
	"name": "alert",
	"doc": "sample avro alert schema v1.0",
	"fields": [
		{"name": "alertId", "type": "long", "doc": "add descriptions like this"},
		{"name": "l1dbId", "type": "long"},
		{"name": "diaSource", "type": "lsst.alert.diaSource"},
		{"name": "prv_diaSources", "type": [{
				"type": "array",
				"items": "lsst.alert.diaSource"}, "null" ], "default": null},
		{"name": "Object", "type": ["lsst.Object", "null"], "default": null},
		{"name": "ssObject", "type": ["lsst.ssObject", "null"], "default": null},
		{"name": "ObjectL2", "type": ["lsst.Object", "null"], "default": null},
		{"name": "diaSourcesL2", "type": [{
				"type": "array",
				"items": "lsst.alert.diaSource"}, "null"], "default": null},
		{"name": "cutoutDifference", "type": ["lsst.alert.cutout", "null"], "default": null},
		{"name": "cutoutTemplate", "type": ["lsst.alert.cutout", "null"], "default": null}
			]
    }


Full example schemas for LSST alerts can be found in the GitHub repository at
https://github.com/lsst-dm/sample-avro-alert.
These were generated using the fields in the ``cat`` package and defined
by the Data Products Definition Document (LSE-163, DPDD).
Avro files with realistic data generated by the Simulations team are
available in the data directory of the GitHub repository at
https://github.com/lsst-dm/alert_stream.



Alert Distribution
==================

Alert distribution uses Apache Kafka
(`https://kafka.apache.org <https://kafka.apache.org>`_),
an open source streaming platform
that can be used for real-time and continuous data pipelines.
Kafka is a scalable pub/sub message queue based on a commit log.
It is used in production at scale at companies such as LinkedIn,
Netflix, and Microsoft to process over 1 trillion messages per day.

Kafka collects messages from processes called "producers,"
which are organized into distinct streams called "topics."
Downstream "consumers" pull messages by subscribing to topics.
Topics can be split into "partitions" that may be distributed
across multiple machines and allow consumers to read in
parallel as "consumer groups."
Data can be replicated by deploying Kafka in cluster mode over several
servers called "brokers."
We will refer to these brokers below as "Kafka brokers" to distinguish
from the LSST alert downstream "community brokers" that will process
LSST alerts.

For LSST alert distribution, Kafka and the accompanying Zookeeper
can be deployed as Docker containers from the DockerHub image repository
maintained by Confluent Inc., the team that created Kafka.
The latest release of ``alert_stream`` uses Kafka and Zookeeper from
Confluent platform release 4.1.1.
The producer used for generating and sending data to Kafka and
template scripts for consumers of the stream are provided in the GitHub
repository at https://github.com/lsst-dm/alert_stream,
which can also be built as a Docker image and deployed as containers.
`DMTN-028 <https://dmtn-028.lsst.io>`_ provides details about benchmarking
deployment of the different components.

Alert Filtering
================

Alert filters can be written using simple Python functions
acting solely on the contents of the alert packets, as
described in the Data Products Definition Document.
Using Python allows scientific users to write complex
functions that are more powerful and simpler to write than
equivalent filters in alternatives such as SQL.
If permitted, scientists could also potentially use modules outside of the
standard library to deploy, e.g., machine learning algorithms on alerts.
User-provided code, however, presents challenges.
The filter submission process needs to be controlled.
Access to the host system needs to be restricted to prevent malicious
activity and security concerns.
To mitigate the concerns of user-provided code, in this design
each filter runs separately and isolated in its own container.
The filter is only allowed access to the approved libraries available in
the Python environment provided in the container, and permissions
of the container can be restricted so as to minimize effects on the host.

A filter is constructed as a consumer of the Kafka topic containing the
full stream and a producer back to a Kafka topic for the filtered stream.
This filtered stream can then be read by another consumer or directed
to another output method for storage.
The downstream consumer of the filtered stream can itself be another
filter, meaning that this scheme allows simple chaining of filters.

Currently, filters are defined in ``lsst.alert.stream.filters`` in the
alert_stream repository.
Each filter is constructed as a class with a filter function.
Below is an example of code defining a simple filter.

.. code-block:: Python

  class Filter001(AlertFilter):
      def filter(self, alert):
          if ((alert['diaSource']['snr'] > 5) &
              (alert['diaSource']['diffFlux'] > 0.00003631)):  # 20th mag
              return True
          else:
              return False

Filters inherit from an AlertFilter base class.
When the filter class is called on each alert, the visit ID is read.
Up to 20 alerts per visit that pass the filter, i.e., return True, are
forwarded to a topic for the filtered alerts, named for the class.
In the above example, alerts are sent to a new topic named "Filter001."
The filter detects the beginning of a new visit when the visit ID
changes from one alert to the next.
This assumes that all alerts from a visit will be received before
the next visit's alerts arrive.
Otherwise, more than 20 alerts may pass through the filter.

An excess of 20 alerts is not cause for concern but is defined by
`numBrokerAlerts` (Number of full-sized alerts that can be received
per visit per user) in performance requirement DMS-REQ-0343 which
was put in place as a coarse load-balancing measure.
Raising this limit would potentially produce scientific gains as a larger
number of alerts of interest would be available to scientific users.
Currently, the first 20 alerts passing the filter are made available
to users, and there is no ability to rank order or otherwise choose
the most scientifically interesting 20 filtered alerts.
Increasing the limit, however, would also increase the network
bandwidth necessary to accommodate larger filtered streams.
The current requirement for `numBrokerUsers` (Supported number of simultaneous
users connected to the LSST alert filtering system) is set at 100,
meaning that the system is at least required to support the
bandwidth needed for 100 filtered streams with 20 alerts per visit.
Increasing the filtered alert limit could then mean that fewer
users would be able to be supported due to networking limitations.
DMS-REQ-0343 notes that the requirement could also be satisfied
by cutting down the content of filtered alerts and potentially
removing fields to decrease the size of filtered streams.



Alert Database
==============

Deployment
===========

The deployment of the alert distribution and mini-broker described here differs
from the prototype described in `DMTN-081 <https://dmtn-081.lsst.io>`_
and has been revised with significant improvement in design.
DMTN-081 describes a system in which groups of filters are deployed
in a single container, sharing one copy of the full alert stream
and utilizing Python's multiprocessing module to parallelize the filters.
Having a group of filters share a copy of the full alert stream
minimizes the number of full streams and the amount of data
that needs to be pulled from Kafka, which is necessary given
a cap on the network bandwidth available for the system.
Additionally, the load of a large number (~10) of consumers,
i.e. a large number of full streams, may cause filters to lag
and not be able to keep up with the volume and real-time velocity of alerts.
(See DMTN-028.)

The previous design deploys filters in groups in order to avoid the
bandwidth needed to otherwise support the design of a one-stream-per-filter
(and therefore 100+ streams) on a single Kafka instance.
The major drawback in this deployment is that groups of filters are running
within a single container and within a single executed Python script.
Filters cannot be isolated to their own environments, and one filter
may slow or otherwise affect another.

The design described here attempts to avoid both the bandwidth issue
and the issue of shared filter environments by placing
components methodically on the hardware available to the system.
This design separates each filter into its own Docker container so that
filters should have no effect on each other.
However, instead of having filter containers deployed on potentially
many separate nodes all consuming streams from one Kafka instance running
on its own dedicated node, a system of downstream Kafka mirrors are deployed
on several nodes from which co-located filters can read streams locally,
cutting down on the data throughput between nodes.
Filters run in separate Docker containers which are placed on the
same node as a Kafka instance mirroring the full stream pulled from the
upstream central Kafka hub where alerts are sent.
The number of mirrored Kafka instances / groups of filters able to
be supported is then determined by the node-to-node available bandwidth,
but the filters reading from the downstream local Kafka instances
can take advantage of the internal bandwidth of the node on which they run.

The updated deployment plan of the alert distribution system
including filtering with the mini-broker is shown in the figure below.
Content of the alerts is created in science pipelines, potentially
parallelized by CCD.
In the alert_stream repo, AlertProducers serialize Python dicts
into Avro format when alerts are sent to a central Kafka broker,
or cluster of Kafka brokers acting as one unit.
For testing, a single AlertProducer can be deployed from the
alert_stream repo that sends local Avro alert visit files to Kafka.
Each Kafka broker in this central hub is deployed on its own
node either with Zookeeper on the same node or Zookeeper on a separate
node with no other services running.
The main Kafka system streams to and feeds downstream community broker
consumers and sinks to the alert database.


.. figure:: deployment-diagram.png


The central Kafka system feeds the mini-broker filtering system,
which is made up of several independent nodes each running a local
instance of Kafka and Zookeeper.
A MirrorMaker instance also runs on each of these nodes and independently
sets up the local mirror of the full alert stream.
Filters are deployed in separate Docker containers for each
on the same node as the local Kafka hub.
In testing on AWS, up to 50 filters may run on each node,
using m4.4xlarge instances.
An m4.4xlarge instance has 14 vCPU, 64 GiB memory, and 2,000 Mbps
dedicated bandwidth to its Elastic Block Store SSD storage volume.

The alert_stream code contains a file of filter classes,
as described above.
In the deployment scripts, a filterStream.py file is included
that takes as input the Kafka broker ip to connect to (i.e,
the address of the local Kafka instance), the topic name of the
full stream of alerts to filter, and the number of the filter
in the list of filter classes to run.
For example, running a container with the command

.. code-block:: Python

    python filterStream.py kafka:9092 full-stream 7

will deploy the seventh filter in the list of filter classes.
Here the classes included are labeled ``Filter001`` - ``Filter100``,
writing to filtered topics of the same name, but these names
are flexible.

The local Kafka instances used for filtering feed downstream consumer users.
In the deployment scripts, a consumer is started in a separate container
for each filtered stream.
These consumers are deployed on separate nodes from the filtering nodes.
Up to 50 consumers have been tested per node on the same type of
instance as the filtering nodes, m4.4xlarge.

Deployment scripts for deploying a full mini-broker configuration
(a producer, central Kafka instance, filtering Kafka instances,
filters, and consumers) are available in the alert_stream repo.
These scripts are specifically for a deployment using Docker Swarm.
As input, files listing the node IDs on which to run the different
components are needed.
The deployment will run 20 filters per node, and 100 total filters
are included.
Complete instructions for deploying on an AWS CloudFormation cluster
are included with the deployment scripts in the swarm directory
of alert_stream.


.. .. rubric:: References

.. Make in-text citations with: :cite:`bibkey`.

.. .. bibliography:: local.bib lsstbib/books.bib lsstbib/lsst.bib lsstbib/lsst-dm.bib lsstbib/refs.bib lsstbib/refs_ads.bib
..    :encoding: latex+latin
..    :style: lsst_aa
