..
  Technote content.

  See https://developer.lsst.io/restructuredtext/style.html
  for a guide to reStructuredText writing.

  Do not put the title, authors or other metadata in this document;
  those are automatically added.

  Use the following syntax for sections:

  Sections
  ========

  and

  Subsections
  -----------

  and

  Subsubsections
  ^^^^^^^^^^^^^^

  To add images, add the image file (png, svg or jpeg preferred) to the
  _static/ directory. The reST syntax for adding the image is

  .. figure:: /_static/filename.ext
     :name: fig-label

     Caption text.

   Run: ``make html`` and ``open _build/html/index.html`` to preview your work.
   See the README at https://github.com/lsst-sqre/lsst-technote-bootstrap or
   this repo's README for more info.

   Feel free to delete this instructional comment.

:tocdepth: 1

.. Please do not modify tocdepth; will be fixed when a new Sphinx theme is shipped.

.. sectnum::

.. TODO: Delete the note below before merging new content to the master branch.

.. note::

   **This technote is not yet published.**

Abstract
=========

We describe the design and implementation of the LSST Alert Distribution System, which provides rapid dissemination of alerts as well as simple filtering.



Alert Serialization
===================

Alerts are packaged using Apache Avro.
Avro is a framework for data serialization in a compact binary format.
It has been used at scale in both industry and science, and it is the
recommended format for data streamed with Apache Kafka.
Avro is more structured in format than JSON or XML, the currently used
format of VOEvent 2.0.
Libraries for reading and writing Avro are available in many languages,
including Python.

To create an Avro alert packet, data is serialized using a JSON schema,
which defines the data types for each field.
Strict adherence to the schema ensures that data will be correctly
interpreted upon receipt.
Alerts in Avro format can be shipped with or without a schema embedded,
allowing them to be more light weight.
The schema used for deserializing packets can also be different from the
schema used for writing, which enables receivers to easily remove
or add new fields.
Files can be included in Avro packets as data type "bytes," making it
possible to embed postage stamp cutouts of detected difference image
sources in a much more compact way than the current VOEvent standard.

Avro schemas can be composed of nested sub-schemas under a top
level namespace.
Nesting simplifies what would otherwise be monolithic schemas
as new fields are added.
For example, the base alert schema (``lsst.alert``) is of type
"record" and includes previous detections of DIA sources as an array
of type ``lsst.alert.diaSource``.

The following shows the top level alert schema:

.. code-block:: JSON

  {
	"namespace": "lsst",
	"type": "record",
	"name": "alert",
	"doc": "sample avro alert schema v1.0",
	"fields": [
		{"name": "alertId", "type": "long", "doc": "add descriptions like this"},
		{"name": "l1dbId", "type": "long"},
		{"name": "diaSource", "type": "lsst.alert.diaSource"},
		{"name": "prv_diaSources", "type": [{
				"type": "array",
				"items": "lsst.alert.diaSource"}, "null" ], "default": null},
		{"name": "diaObject", "type": ["lsst.diaObject", "null"], "default": null},
		{"name": "ssObject", "type": ["lsst.ssObject", "null"], "default": null},
		{"name": "diaObjectL2", "type": ["lsst.diaObject", "null"], "default": null},
		{"name": "diaSourcesL2", "type": [{
				"type": "array",
				"items": "lsst.alert.diaSource"}, "null"], "default": null},
		{"name": "cutoutDifference", "type": ["lsst.alert.cutout", "null"], "default": null},
		{"name": "cutoutTemplate", "type": ["lsst.alert.cutout", "null"], "default": null}
			]
    }


Full example schemas for LSST alerts can be found in the GitHub repository at
https://github.com/lsst-dm/sample-avro-alert.
These were generated using the fields in the ``cat`` package and defined
by the Data Products Definition Document (LSE-163, DPDD).
Avro files with realistic data generated by the Simulations team are
available in data directory of the GitHub repository at
https://github.com/lsst-dm/alert_stream.



Alert Distribution
==================

Alert distribution uses Apache Kafka, an open source streaming platform
that can be used for real-time and continuous data pipelines.
Kafka is a scalable pub/sub message queue based on a commit log.
It is used in production at scale at companies such as LinkedIn,
Netflix, and Microsoft to process over 1 trillion messages per day.

Kafka collects messages from processes called "producers,"
which are organized into distinct streams called "topics."
Downstream "consumers" pull messages by subscribing to topics.
Topics can be split into "partitions" that may be distributed
across multiple machines and allow consumers to read in
parallel as "consumer groups."
Data can be replicated by deploying Kafka in cluster mode over several
servers called "brokers."
We will refer to these brokers below as "Kafka brokers" to distinguish
from the LSST alert downstream "community brokers."

For LSST alert distribution, Kafka and the accompanying Zookeeper
can be deployed as Docker containers from the DockerHub image repository
maintained by Confluent Inc., the team that created Kafka.
The latest release of ``alert_stream`` uses Kafka and Zookeeper from
Confluent platform release 4.1.1.
The producer used for generating and sending data to Kafka and
template scripts for consumers of the stream are provided in the GitHub
repository at https://github.com/lsst-dm/alert_stream,
which can also be built as a Docker image and deployed as containers.

Alert Filtering
================

Alert filters can be written using simple Python functions that act
solely on the contents of the alert packets.
Each filter can run separately and isolated in its own container.
A filter is constructed as a consumer of the Kafka topic containing the
full stream and a producer back to a Kafka topic for the filtered stream.
This filtered stream can then be read by another consumer or directed
to another output method for storage.

Currently, filters are defined in ``lsst.alert.stream.filters`` in the
alert_stream repository.
Each filter is constructed as a class with a filter function.
Below is an example of code defining a simple filter.

.. code-block:: Python

  class Filter001(AlertFilter):
      def filter(self, alert):
          if ((alert['diaSource']['snr'] > 5) &
              (alert['diaSource']['diffFlux'] > 0.00003631)):  # 20th mag
              return True
          else:
              return False

Filters inherit from an AlertFilter base class.
When the filter class is called on each alert, the visit ID is read.
Up to 20 alerts per visit that pass the filter, i.e., return True, are
forwarded to a topic for the filtered alerts, named for the class.
In the above example, alerts are sent to a new topic named "Filter001."
The filter detects the beginning of a new visit when the visit ID
changes from one alert to the next.
This assumes that all alerts from a visit will be received before
the next visit's alerts arrive.
Otherwise, more than 20 alerts may pass through the filter.


Alert Database
==============

Deployment
===========

Deployment of the alert distribution system including filtering with
the mini-broker is shown in the figure below.
Content of the alerts is created in science pipelines, potentially
parallelized by CCD.
In the alert_stream repo, AlertProducers serialize Python dicts
into Avro format when alerts are sent to a central Kafka broker,
or cluster of Kafka brokers acting as one unit.
For testing, a single AlertProducer can be deployed from the
alert_stream repo that sends local Avro alert visit files to Kafka.
Each Kafka broker in this central hub is deployed on its own
node either with Zookeeper on the same node or Zookeeper on a separate
node with no other services running.
The main Kafka system streams to and feeds downstream community broker
consumers and sinks to the alert database.


.. figure:: deployment-diagram.png


The central Kafka system feeds the mini-broker filtering system,
which is made up of several independent nodes each running a local
instance of Kafka and Zookeeper.
A MirrorMaker instance also runs on each of these nodes and independently
sets up the local mirror of the full alert stream.
Filters are deployed in separate Docker containers for each
on the same node as the local Kafka hub.
In testing on AWS, up to 50 filters may run on each node,
using m4.4xlarge instances.
An m4.4xlarge instance has 14 vCPU, 64 GiB memory, and 2,000 Mbps
dedicated bandwidth to its Elastic Block Store SSD storage volume.

The alert_stream code contains a file of filter classes,
as described above.
In the deployment scripts, a filterStream.py file is included
that takes as input the Kafka broker ip to connect to (i.e,
the address of the local Kafka instance), the topic name of the
full stream of alerts to filter, and the number of the filter
in the list of filter classes to run.
For example, running a container with the command

.. code-block:: Python

    python filterStream.py kafka:9092 full-stream 7

will deploy the seventh filter in the list of filter classes.
Here the classes included are labeled ``Filter001`` - ``Filter100``,
writing to filtered topics of the same name, but these names
are flexible.

The local Kafka instances used for filtering feed downstream consumer users.
In the deployment scripts, a consumer is started in a separate container
for each filtered stream.
These consumers are deployed on separate nodes from the filtering nodes.
Up to 50 consumers have been tested per node on the same type of
instance as the filtering nodes, m4.4xlarge.

Deployment scripts for deploying a full mini-broker configuration
(a producer, central Kafka instance, filtering Kafka instances,
filters, and consumers) are available in the alert_stream repo.
These scripts are specifically for a deployment using Docker Swarm.
As input, files listing the node IDs on which to run the different
components are needed.
The deployment will run 20 filters per node, and 100 total filters
are included.
Complete instructions for deploying on an AWS CloudFormation cluster
are included with the deployment scripts in the swarm directory
of alert_stream.


.. .. rubric:: References

.. Make in-text citations with: :cite:`bibkey`.

.. .. bibliography:: local.bib lsstbib/books.bib lsstbib/lsst.bib lsstbib/lsst-dm.bib lsstbib/refs.bib lsstbib/refs_ads.bib
..    :encoding: latex+latin
..    :style: lsst_aa
